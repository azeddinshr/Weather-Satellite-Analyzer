{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cbadf92c-7664-4033-95f3-372f04c6d591",
      "metadata": {},
      "source": [
        "# LLaVA-OneVision Fine-Tuning for Weather Satellite Analysis\n",
        "\n",
        "## Overview\n",
        "This notebook demonstrates fine-tuning the **LLaVA-OneVision-7B-OV** vision-language model for weather satellite image analysis using advanced Parameter-Efficient Fine-Tuning (PEFT) techniques.\n",
        "\n",
        "### Key Features\n",
        "- **Model**: LLaVA-OneVision-7B-OV\n",
        "- **Techniques**: DoRA, AdaLoRA, and QLoRA\n",
        "- **Dataset**: Weather Satellite Images with captions\n",
        "- **Framework**: HuggingFace Transformers + TRL + PEFT\n",
        "\n",
        "### What is LLaVA-OneVision?\n",
        "LLaVA-OneVision is a state-of-the-art vision-language model that:\n",
        "- Understands and generates natural language about images\n",
        "- Combines visual perception with language understanding\n",
        "- Provides detailed descriptions of visual content\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abb10bab-3a5a-4767-9eae-06303c527657",
      "metadata": {},
      "source": [
        "## 1. Environment Setup & Installation\n",
        "\n",
        "### Kaggle Dataset Import\n",
        "Import the weather satellite dataset from Kaggle Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f9f1293-7d8f-491c-bbc5-14522e4b2c76",
      "metadata": {},
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "azdinsahir_wsi_data_pfe_path = kagglehub.dataset_download('azdinsahir/wsi-data-pfe')\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3849a024-e01d-4aee-8487-df3a800e456f",
      "metadata": {},
      "source": [
        "### Install Required Libraries\n",
        "Installing LLaVA-OneVision dependencies including:\n",
        "- `transformers`: HuggingFace library for model loading\n",
        "- `bitsandbytes`: For 4-bit quantization\n",
        "- `peft`: Parameter-Efficient Fine-Tuning library\n",
        "- `trl`: Transformer Reinforcement Learning\n",
        "- `wandb`: Experiment tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb74f8cf-5178-4a96-989c-ac57c3017466",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
            "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
            "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# ====================================================================\n",
        "# CELL 1: LLaVA-OneVision Setup\n",
        "# ====================================================================\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q transformers>=4.44.0\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q bitsandbytes accelerate\n",
        "!pip install -q peft trl\n",
        "!pip install -q wandb\n",
        "!pip install -q pillow numpy\n",
        "!pip install -q sentencepiece protobuf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b13458af-8b8e-4d2d-bfe9-85753a41f84a",
      "metadata": {},
      "source": [
        "### Weights & Biases Authentication\n",
        "Login to W&B for experiment tracking and metrics logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baa62508-f0c5-43a7-a5b1-9c3b9c0bfa6a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!wandb login YOUR_WANDB_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6b966e3-255c-4e5b-a9ca-fd4d511aa6bc",
      "metadata": {},
      "source": [
        "## 2. Imports & Configuration\n",
        "\n",
        "### Import Libraries and Set Training Parameters\n",
        "Importing all required libraries and configuring:\n",
        "- Model paths and names\n",
        "- Training hyperparameters\n",
        "- Quantization settings\n",
        "- LoRA/DoRA/AdaLoRA configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a36d77f9-af16-402d-86f1-d9c653ee4996",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… HF token loaded from secrets\n",
            "ğŸš€ Setup completed!\n"
          ]
        }
      ],
      "source": [
        "# ====================================================================\n",
        "# CELL 2: Imports and Configuration\n",
        "# ====================================================================\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import wandb\n",
        "import gc\n",
        "import time\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from transformers import (\n",
        "    LlavaOnevisionForConditionalGeneration,\n",
        "    LlavaOnevisionProcessor,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoProcessor\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, AdaLoraConfig\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "from huggingface_hub import HfApi\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configuration\n",
        "DATASET_PATH = \"/kaggle/input/wsi-data-pfe/wsi-data-r/\"  # UPDATE THIS\n",
        "HF_USERNAME = \"azdin\"  # UPDATE THIS\n",
        "MODEL_ID = \"llava-hf/llava-onevision-qwen2-7b-ov-hf\"\n",
        "MAX_SAMPLES = 200  # Adjust \n",
        "\n",
        "# Get HF token from Colab secrets\n",
        "try:\n",
        "    HF_TOKEN = 'ur-token-here'  # Replace with your actual token or method to retrieve it\n",
        "    print(\"âœ… HF token loaded from secrets\")\n",
        "except:\n",
        "    print(\"âŒ Please add 'hf_token' to Colab secrets\")\n",
        "    HF_TOKEN = None\n",
        "\n",
        "# Set environment\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "print(\"ğŸš€ Setup completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16d61a08-dc59-417c-bbb9-1fa94a5c70c1",
      "metadata": {},
      "source": [
        "### Utility Functions\n",
        "Helper functions for:\n",
        "- Memory management and GPU monitoring\n",
        "- Dataset loading and preprocessing\n",
        "- Model training and evaluation\n",
        "- Inference and visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "318776e1-a742-45da-a6e3-e62326d4f472",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ› ï¸ Utility functions loaded!\n"
          ]
        }
      ],
      "source": [
        "# ====================================================================\n",
        "# CELL 3: Utility Functions\n",
        "# ====================================================================\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Clear GPU memory\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "    print(f\"GPU allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n",
        "def load_weather_dataset(dataset_path, max_samples=None):\n",
        "    \"\"\"Load weather satellite dataset with metadata\"\"\"\n",
        "\n",
        "    print(f\"ğŸ“‚ Loading weather dataset from: {dataset_path}\")\n",
        "\n",
        "    # Load manifest\n",
        "    manifest_path = os.path.join(dataset_path, \"training_manifest.json\")\n",
        "    with open(manifest_path, 'r') as f:\n",
        "        manifest = json.load(f)\n",
        "\n",
        "    # Use all samples or limit\n",
        "    if max_samples is None:\n",
        "        data = manifest['training_data']\n",
        "        print(f\"Using ALL {len(data)} samples\")\n",
        "    else:\n",
        "        data = manifest['training_data'][:max_samples]\n",
        "        print(f\"Limited to {len(data)} samples (max_samples={max_samples})\")\n",
        "\n",
        "    images_dir = os.path.join(dataset_path, \"images\")\n",
        "    captions_dir = os.path.join(dataset_path, \"captions\")\n",
        "    metadata_dir = os.path.join(dataset_path, \"metadata\")\n",
        "\n",
        "    print(f\"Processing {len(data)} samples with metadata integration...\")\n",
        "\n",
        "    def extract_temporal_context(metadata):\n",
        "        \"\"\"Extract temporal information from metadata\"\"\"\n",
        "        temporal_info = {}\n",
        "\n",
        "        if 'temporal_info' in metadata:\n",
        "            temp_data = metadata['temporal_info']\n",
        "            temporal_info = {\n",
        "                'date': temp_data.get('date_formatted', 'Unknown date'),\n",
        "                'season': temp_data.get('season', 'Unknown season'),\n",
        "                'month': temp_data.get('month_name', 'Unknown month')\n",
        "            }\n",
        "\n",
        "        if 'image_info' in metadata and 'capture_date' in metadata['image_info']:\n",
        "            temporal_info['date'] = metadata['image_info']['capture_date']\n",
        "\n",
        "        return temporal_info\n",
        "\n",
        "    def extract_weather_context(metadata):\n",
        "        \"\"\"Extract weather information\"\"\"\n",
        "        weather_info = []\n",
        "\n",
        "        if 'weather_descriptions' in metadata:\n",
        "            for region_key, region_data in metadata['weather_descriptions'].items():\n",
        "                raw_data = None\n",
        "\n",
        "                if 'raw_data' in region_data:\n",
        "                    raw_data = region_data['raw_data']\n",
        "                elif 'regional_weather' in metadata and region_key in metadata['regional_weather']:\n",
        "                    raw_data = metadata['regional_weather'][region_key].get('weather_12utc', {})\n",
        "\n",
        "                weather_entry = {\n",
        "                    'region': region_key,\n",
        "                    'location': region_data.get('location', region_key),\n",
        "                    'description': region_data.get('description', 'No description'),\n",
        "                    'raw_data': raw_data or {}\n",
        "                }\n",
        "                weather_info.append(weather_entry)\n",
        "\n",
        "        return weather_info\n",
        "\n",
        "    def extract_city_coordinates(metadata):\n",
        "        \"\"\"Extract city coordinates\"\"\"\n",
        "        cities_info = []\n",
        "        if 'regional_weather' in metadata:\n",
        "            for city_key, city_data in metadata['regional_weather'].items():\n",
        "                if 'coordinates' in city_data:\n",
        "                    coords = city_data['coordinates']\n",
        "                    location_desc = city_data.get('location', city_key)\n",
        "                    cities_info.append({\n",
        "                        'name': city_key.replace('_', ' ').title(),\n",
        "                        'description': location_desc,\n",
        "                        'lat': coords['lat'],\n",
        "                        'lon': coords['lon']\n",
        "                    })\n",
        "        return cities_info\n",
        "\n",
        "    def format_weather_data(weather_context):\n",
        "        \"\"\"Format weather data table\"\"\"\n",
        "        if not weather_context:\n",
        "            return \"Weather data unavailable\"\n",
        "\n",
        "        weather_table = \"**SURFACE METEOROLOGICAL CONDITIONS (at 12:00 Noon UTC):**\\n\"\n",
        "        weather_table += \"| Location | Temp(Â°C) | Humidity(%) | Wind(m/s) | Precip(mm) | Cloud(%) | Pressure(kPa) |\\n\"\n",
        "        weather_table += \"|----------|----------|-------------|-----------|------------|----------|---------------|\\n\"\n",
        "\n",
        "        for region_data in weather_context:\n",
        "            if isinstance(region_data, dict) and 'raw_data' in region_data:\n",
        "                raw = region_data['raw_data']\n",
        "                location = region_data.get('location', 'Unknown').split(',')[0]\n",
        "                temp = f\"{raw.get('T2M', 'N/A'):.1f}\" if raw.get('T2M') != 'N/A' else 'N/A'\n",
        "                humidity = f\"{raw.get('RH2M', 'N/A'):.0f}\" if raw.get('RH2M') != 'N/A' else 'N/A'\n",
        "                wind = f\"{raw.get('WS10M', 'N/A'):.1f}\" if raw.get('WS10M') != 'N/A' else 'N/A'\n",
        "                precip = f\"{raw.get('PRECTOTCORR', 'N/A'):.1f}\" if raw.get('PRECTOTCORR') != 'N/A' else 'N/A'\n",
        "                cloud = f\"{raw.get('CLOUD_AMT', 'N/A'):.0f}\" if raw.get('CLOUD_AMT') != 'N/A' else 'N/A'\n",
        "                pressure = f\"{raw.get('PS', 'N/A'):.0f}\" if raw.get('PS') != 'N/A' else 'N/A'\n",
        "\n",
        "                weather_table += f\"| {location} | {temp} | {humidity} | {wind} | {precip} | {cloud} | {pressure} |\\n\"\n",
        "\n",
        "        return weather_table\n",
        "\n",
        "    def build_enhanced_system_prompt(metadata):\n",
        "        \"\"\"Build enhanced prompt used in GPT-4V collection\"\"\"\n",
        "\n",
        "        temporal_context = extract_temporal_context(metadata)\n",
        "        weather_context = extract_weather_context(metadata)\n",
        "        weather_table = format_weather_data(weather_context)\n",
        "        cities_info = extract_city_coordinates(metadata)\n",
        "\n",
        "        # Build city reference section\n",
        "        city_references = \"**MOROCCAN CITIES (for geographic reference):**\\n\"\n",
        "        for city in cities_info:\n",
        "            city_name = city['name'].replace('_', ' ').title()\n",
        "            if not any(word in city_name.lower() for word in ['mountains', 'sahara', 'desert', 'atlas', 'rif']):\n",
        "                city_references += f\"- **{city_name}** ({city['lat']:.2f}Â°N, {city['lon']:.2f}Â°W): {city['description']}\\n\"\n",
        "\n",
        "        date_info = temporal_context.get('date', 'Unknown date')\n",
        "        season_info = temporal_context.get('season', 'Unknown season')\n",
        "        month_info = temporal_context.get('month', 'Unknown month')\n",
        "\n",
        "        system_prompt = f\"\"\"You are a professional meteorologist and satellite imagery analyst with expertise in North African climatology and Morocco's atmospheric patterns, trained in international meteorological standards (WMO/NOAA/EUMETSAT protocols).\n",
        "\n",
        "**SATELLITE ACQUISITION DETAILS:**\n",
        "- **Date & Time**: {date_info} at 12:00 Noon UTC (Midday acquisition)\n",
        "- **Geographic Domain**: Morocco, North Africa\n",
        "- **Seasonal Context**: {season_info} ({month_info})\n",
        "- **Satellite**: MODIS Terra - Corrected Reflectance True Color\n",
        "- **Spatial Resolution**: 250 meters\n",
        "\n",
        "{weather_table}\n",
        "\n",
        "{city_references}\n",
        "\n",
        "**ANALYSIS FRAMEWORK:**\n",
        "Following standard meteorological satellite interpretation protocols for comprehensive weather analysis including:\n",
        "\n",
        "1. **TEMPORAL AND GEOGRAPHIC CONTEXT:** Reference exact acquisition date, noon timing, and specific Moroccan cities\n",
        "2. **SURFACE METEOROLOGICAL CONDITIONS:** Integrate ground-level weather measurements with satellite observations\n",
        "3. **ATMOSPHERIC PHENOMENA IDENTIFICATION:** Cloud field analysis, weather systems, and dust analysis if visible\n",
        "4. **CITY-FOCUSED WEATHER ANALYSIS:** Link atmospheric conditions to specific cities with ground-level measurements\n",
        "5. **METEOROLOGICAL INTERPRETATION:** Atmospheric stability assessment and moisture distribution patterns\n",
        "\n",
        "**CRITICAL INSTRUCTIONS:**\n",
        "- ALWAYS begin with date and 12:00 noon UTC acquisition time\n",
        "- ALWAYS reference specific Moroccan CITIES by name (not regions)\n",
        "- ALWAYS specify \"ground-level\" or \"surface\" when mentioning weather measurements\n",
        "- Keep descriptions concise, professional, and focused on observable atmospheric phenomena\n",
        "\n",
        "Provide comprehensive meteorological analysis focusing on cities and ground-level weather integration.\"\"\"\n",
        "\n",
        "        return system_prompt\n",
        "\n",
        "    def format_weather_sample_llava(item):\n",
        "        \"\"\"Format sample for LLaVA-OneVision\"\"\"\n",
        "\n",
        "        # Load image\n",
        "        image_path = os.path.join(images_dir, item['image_file'])\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "        # Load caption\n",
        "        caption_path = os.path.join(captions_dir, item['caption_file'])\n",
        "        with open(caption_path, 'r', encoding='utf-8') as f:\n",
        "            caption = f.read().strip()\n",
        "\n",
        "        # Load metadata and create enhanced system prompt\n",
        "        system_message = \"\"\"You are an expert meteorologist analyzing weather satellite imagery. Provide detailed, professional analysis of atmospheric conditions visible in MODIS Terra satellite images of Morocco.\"\"\"\n",
        "\n",
        "        if item.get('has_metadata') and item.get('metadata_file'):\n",
        "            metadata_path = os.path.join(metadata_dir, item['metadata_file'])\n",
        "            try:\n",
        "                with open(metadata_path, 'r') as f:\n",
        "                    metadata = json.load(f)\n",
        "\n",
        "                system_message = build_enhanced_system_prompt(metadata)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Could not load metadata for {item['image_file']}: {e}\")\n",
        "\n",
        "        # Format for LLaVA-OneVision\n",
        "        # Combine system message with user query\n",
        "        full_user_message = f\"{system_message}\\n\\nPlease analyze this MODIS Terra satellite image of Morocco and provide a comprehensive meteorological analysis using the provided weather data and geographic context.\"\n",
        "\n",
        "        return {\n",
        "            \"messages\": [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": full_user_message},\n",
        "                        {\"type\": \"image\"}\n",
        "                    ],\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": caption}\n",
        "                    ],\n",
        "                }\n",
        "            ],\n",
        "            \"images\": [image]\n",
        "        }\n",
        "\n",
        "    # Format all samples\n",
        "    formatted_data = []\n",
        "    for i, item in enumerate(data):\n",
        "        try:\n",
        "            sample = format_weather_sample_llava(item)\n",
        "            formatted_data.append(sample)\n",
        "            if i % 50 == 0:\n",
        "                print(f\"  Processed {i}/{len(data)} samples...\")\n",
        "        except Exception as e:\n",
        "            print(f\"  âš ï¸ Skipped sample {i}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"âœ… Successfully formatted {len(formatted_data)} samples with metadata\")\n",
        "    return formatted_data\n",
        "\n",
        "def create_data_collator_llava(processor):\n",
        "    \"\"\"Create data collator for LLaVA-OneVision\"\"\"\n",
        "    def collate_fn(examples):\n",
        "        # Extract messages and images\n",
        "        texts = []\n",
        "        images = []\n",
        "\n",
        "        for example in examples:\n",
        "            # Apply chat template\n",
        "            text = processor.apply_chat_template(\n",
        "                example[\"messages\"],\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=False\n",
        "            )\n",
        "            texts.append(text)\n",
        "            images.append(example[\"images\"][0])\n",
        "\n",
        "        # Process batch\n",
        "        batch = processor(\n",
        "            text=texts,\n",
        "            images=images,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True\n",
        "        )\n",
        "\n",
        "        # Create labels\n",
        "        labels = batch[\"input_ids\"].clone()\n",
        "        labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        # Mask image tokens in labels\n",
        "        image_token_id = processor.tokenizer.convert_tokens_to_ids(\"<image>\")\n",
        "        labels[labels == image_token_id] = -100\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "        return batch\n",
        "\n",
        "    return collate_fn\n",
        "\n",
        "def upload_to_huggingface(model_path, repo_name, technique_name):\n",
        "    \"\"\"Upload model to Hugging Face Hub\"\"\"\n",
        "\n",
        "    if not HF_TOKEN:\n",
        "        print(\"âŒ HF token not available, skipping upload\")\n",
        "        return None\n",
        "\n",
        "    hf_repo_name = f\"{HF_USERNAME}/{repo_name}\"\n",
        "\n",
        "    try:\n",
        "        print(f\"ğŸš€ Uploading {technique_name} to HF Hub: {hf_repo_name}\")\n",
        "\n",
        "        # Initialize API\n",
        "        api = HfApi(token=HF_TOKEN)\n",
        "\n",
        "        # Create repo\n",
        "        api.create_repo(\n",
        "            repo_id=hf_repo_name,\n",
        "            exist_ok=True,\n",
        "            token=HF_TOKEN,\n",
        "            private=False,\n",
        "            repo_type=\"model\"\n",
        "        )\n",
        "\n",
        "        # Upload model files\n",
        "        api.upload_folder(\n",
        "            folder_path=model_path,\n",
        "            repo_id=hf_repo_name,\n",
        "            commit_message=f\"Upload LLaVA-OneVision weather satellite {technique_name} adapter\"\n",
        "        )\n",
        "\n",
        "        # Create README\n",
        "        readme_content = f\"\"\"---\n",
        "license: apache-2.0\n",
        "base_model: llava-hf/llava-onevision-qwen2-7b-ov-hf\n",
        "tags:\n",
        "- llava\n",
        "- llava-onevision\n",
        "- weather\n",
        "- satellite\n",
        "- morocco\n",
        "- meteorology\n",
        "- {technique_name.lower()}\n",
        "- fine-tuned\n",
        "---\n",
        "\n",
        "# LLaVA-OneVision Weather Analysis - {technique_name}\n",
        "\n",
        "Fine-tuned using **{technique_name}** technique for weather satellite imagery analysis.\n",
        "\n",
        "## Model Details\n",
        "\n",
        "- **Base Model:** llava-hf/llava-onevision-qwen2-7b-ov-hf\n",
        "- **Technique:** {technique_name}\n",
        "- **Domain:** Weather satellite imagery analysis\n",
        "- **Dataset:** Weather satellite images with meteorological metadata\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from transformers import LlavaOnevisionForConditionalGeneration, AutoProcessor\n",
        "import torch\n",
        "\n",
        "# Load base model\n",
        "model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n",
        "    \"llava-hf/llava-onevision-qwen2-7b-ov-hf\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\")\n",
        "\n",
        "# Load fine-tuned adapter\n",
        "model.load_adapter(\"{hf_repo_name}\")\n",
        "\n",
        "# Use for weather analysis...\n",
        "```\n",
        "\n",
        "## Training Details\n",
        "\n",
        "- **Technique:** {technique_name}\n",
        "- **Quantization:** 4-bit NF4\n",
        "- **Training Data:** Weather satellite imagery with metadata\n",
        "- **Target Modules:** Attention and projection layers\n",
        "\"\"\"\n",
        "\n",
        "        # Upload README\n",
        "        api.upload_file(\n",
        "            path_or_fileobj=readme_content.encode(),\n",
        "            path_in_repo=\"README.md\",\n",
        "            repo_id=hf_repo_name,\n",
        "            commit_message=\"Add README\"\n",
        "        )\n",
        "\n",
        "        print(f\"âœ… Successfully uploaded to: https://huggingface.co/{hf_repo_name}\")\n",
        "        return hf_repo_name\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Upload failed: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"ğŸ› ï¸ Utility functions loaded!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8c63dc5-c550-403d-aad5-8a561fd8b3aa",
      "metadata": {},
      "source": [
        "## 3. Dataset Preparation\n",
        "\n",
        "### Load Weather Satellite Dataset\n",
        "Loading and splitting the dataset into training and evaluation sets.\n",
        "\n",
        "The dataset contains:\n",
        "- Weather satellite images from various sources\n",
        "- Descriptive captions for each image\n",
        "- Meteorological annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f63549f1-3f7c-4d35-a802-c30b37a726e2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“‚ Loading weather dataset...\n",
            "ğŸ“‚ Loading weather dataset from: /kaggle/input/wsi-data-pfe/wsi-data-r/\n",
            "Using ALL 973 samples\n",
            "Processing 973 samples with metadata integration...\n",
            "  Processed 0/973 samples...\n",
            "  Processed 50/973 samples...\n",
            "  Processed 100/973 samples...\n",
            "  Processed 150/973 samples...\n",
            "  Processed 200/973 samples...\n",
            "  Processed 250/973 samples...\n",
            "  Processed 300/973 samples...\n",
            "  Processed 350/973 samples...\n",
            "  Processed 400/973 samples...\n",
            "  Processed 450/973 samples...\n",
            "  Processed 500/973 samples...\n",
            "  Processed 550/973 samples...\n",
            "  Processed 600/973 samples...\n",
            "  Processed 650/973 samples...\n",
            "  Processed 700/973 samples...\n",
            "  Processed 750/973 samples...\n",
            "  Processed 800/973 samples...\n",
            "  Processed 850/973 samples...\n",
            "  Processed 900/973 samples...\n",
            "  Processed 950/973 samples...\n",
            "âœ… Successfully formatted 973 samples with metadata\n",
            "ğŸ“Š Dataset split: 778 train, 195 eval\n"
          ]
        }
      ],
      "source": [
        "# ====================================================================\n",
        "# CELL 4: Load Dataset\n",
        "# ====================================================================\n",
        "\n",
        "# Load dataset once for both techniques\n",
        "print(\"ğŸ“‚ Loading weather dataset...\")\n",
        "dataset = load_weather_dataset(DATASET_PATH, max_samples=None)\n",
        "\n",
        "# Split dataset\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_dataset = dataset[:train_size]\n",
        "eval_dataset = dataset[train_size:]\n",
        "\n",
        "print(f\"ğŸ“Š Dataset split: {len(train_dataset)} train, {len(eval_dataset)} eval\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f28cbaa-1dda-4e10-91a7-1ebdc1952f9c",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Fine-Tuning Techniques\n",
        "\n",
        "### Technique 1: DoRA (Weight-Decomposed Low-Rank Adaptation)\n",
        "\n",
        "#### What is DoRA?\n",
        "DoRA enhances LoRA by decomposing pre-trained weights into two components:\n",
        "- **Magnitude**: The scale/strength of weights\n",
        "- **Direction**: The orientation of weights\n",
        "\n",
        "This decomposition allows the model to learn more effectively by updating both components separately.\n",
        "\n",
        "#### Key Parameters:\n",
        "- **Rank**: 16 (dimensionality of low-rank matrices)\n",
        "- **Alpha**: 32 (scaling factor)\n",
        "- **Target Modules**: q_proj, v_proj, k_proj, o_proj\n",
        "- **Use DoRA**: True\n",
        "\n",
        "#### Advantages:\n",
        "âœ… Better learning capacity than standard LoRA  \n",
        "âœ… More stable training dynamics  \n",
        "âœ… Superior performance on vision-language tasks  \n",
        "âœ… Minimal memory overhead compared to full fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f24addca-a6a3-482d-8850-9f59254f5e60",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ”¥ TECHNIQUE 1: DoRA (Weight-Decomposed LoRA)\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mazdinsahir11\u001b[0m (\u001b[33mazdinsahir11-university-mohamed-v\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/kaggle/working/wandb/run-20250728_142051-6oky4zjn</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/azdinsahir11-university-mohamed-v/llava-onevision-dora-weather/runs/6oky4zjn' target=\"_blank\">weather-satellite-dora</a></strong> to <a href='https://wandb.ai/azdinsahir11-university-mohamed-v/llava-onevision-dora-weather' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/azdinsahir11-university-mohamed-v/llava-onevision-dora-weather' target=\"_blank\">https://wandb.ai/azdinsahir11-university-mohamed-v/llava-onevision-dora-weather</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/azdinsahir11-university-mohamed-v/llava-onevision-dora-weather/runs/6oky4zjn' target=\"_blank\">https://wandb.ai/azdinsahir11-university-mohamed-v/llava-onevision-dora-weather/runs/6oky4zjn</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU allocated: 0.00 GB\n",
            "GPU reserved: 0.00 GB\n",
            "ğŸ“¥ Loading LLaVA-OneVision for DoRA...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bba963d5472d474989d71608cadaabec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f3a418ae7bc4c1d83819dd7b4d522d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6fa0916fe1f49a39a6431e9d858a36e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34e6a4a1db684f65ae448a6bf65a3925",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4630223bfda841dbb76ec793c34000fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "614d7936077b49d691ec0b687aba48d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b565b998098f48d79e88bbeec5b29c8a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/1.23G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d47358450bd4435399d7216825bfda38",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc5aab3eaa2d408fb756293e449c0c94",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/126 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ====================================================================\n",
        "# CELL 5: DoRA Technique Training\n",
        "# ====================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ”¥ TECHNIQUE 1: DoRA (Weight-Decomposed LoRA)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Setup W&B for DoRA\n",
        "dora_config = {\n",
        "    \"technique\": \"DoRA\",\n",
        "    \"model\": \"LLaVA-OneVision\",\n",
        "    \"quantization\": \"4-bit NF4\",\n",
        "    \"lora_rank\": 16,\n",
        "    \"lora_alpha\": 32,\n",
        "    \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "    \"samples\": MAX_SAMPLES,\n",
        "    \"epochs\": 3\n",
        "}\n",
        "\n",
        "wandb.init(\n",
        "    project=\"llava-onevision-dora-weather\",\n",
        "    name=\"weather-satellite-dora\",\n",
        "    config=dora_config,\n",
        "    tags=[\"dora\", \"llava-onevision\", \"weather-satellite\", \"comparison\"]\n",
        ")\n",
        "\n",
        "clear_memory()\n",
        "\n",
        "# Setup quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model\n",
        "print(\"ğŸ“¥ Loading LLaVA-OneVision for DoRA...\")\n",
        "model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "\n",
        "# DoRA config\n",
        "try:\n",
        "    peft_config = LoraConfig(\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        r=16,\n",
        "        bias=\"none\",\n",
        "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        use_dora=True,  # Enable DoRA\n",
        "    )\n",
        "    print(\"âœ… DoRA configuration created\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ DoRA not supported, using enhanced LoRA: {e}\")\n",
        "    peft_config = LoraConfig(\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        r=16,\n",
        "        bias=\"none\",\n",
        "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "# Apply PEFT\n",
        "peft_model = get_peft_model(model, peft_config)\n",
        "peft_model.print_trainable_parameters()\n",
        "\n",
        "# Training arguments for DoRA\n",
        "dora_output_dir = \"./llava_dora_weather_model\"\n",
        "training_args = SFTConfig(\n",
        "    output_dir=dora_output_dir,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    gradient_checkpointing=True,\n",
        "\n",
        "    optim=\"adamw_torch\",\n",
        "    learning_rate=1e-4,  # Lower LR for DoRA\n",
        "    lr_scheduler_type=\"constant\",\n",
        "\n",
        "    logging_steps=10,\n",
        "    eval_steps=max(25, len(train_dataset) // 30),\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=max(50, len(train_dataset) // 20),\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    load_best_model_at_end=True,\n",
        "\n",
        "    bf16=True,\n",
        "    tf32=False,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "\n",
        "    push_to_hub=False,\n",
        "    report_to=\"wandb\",\n",
        "\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    dataset_text_field=\"\",\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
        ")\n",
        "\n",
        "training_args.remove_unused_columns = False\n",
        "\n",
        "# Create trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=create_data_collator_llava(processor),\n",
        "    peft_config=peft_config,\n",
        "    processing_class=processor.tokenizer,\n",
        ")\n",
        "\n",
        "# Train DoRA\n",
        "print(\"ğŸš€ Starting DoRA training...\")\n",
        "dora_start_time = time.time()\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "dora_training_time = time.time() - dora_start_time\n",
        "\n",
        "# Save model locally\n",
        "trainer.save_model(dora_output_dir)\n",
        "\n",
        "# Final evaluation\n",
        "dora_eval_result = trainer.evaluate()\n",
        "\n",
        "print(f\"âœ… DoRA training completed in {dora_training_time/60:.1f} minutes\")\n",
        "print(f\"ğŸ“Š Final DoRA eval loss: {dora_eval_result['eval_loss']:.4f}\")\n",
        "\n",
        "# Upload to HuggingFace\n",
        "dora_hf_repo = upload_to_huggingface(dora_output_dir, \"llava-onevision-weather-dora\", \"DoRA\")\n",
        "\n",
        "# Log final results\n",
        "wandb.log({\n",
        "    \"final_training_time\": dora_training_time,\n",
        "    \"final_eval_loss\": dora_eval_result['eval_loss'],\n",
        "    \"hf_repo\": dora_hf_repo,\n",
        "    \"training_completed\": True\n",
        "})\n",
        "\n",
        "wandb.finish()\n",
        "\n",
        "print(f\"ğŸ’¾ DoRA model saved locally: {dora_output_dir}\")\n",
        "if dora_hf_repo:\n",
        "    print(f\"ğŸŒ DoRA model on HF: https://huggingface.co/{dora_hf_repo}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84a4c3b2-ca01-4b50-8168-9a941502ff8c",
      "metadata": {},
      "source": [
        "### Technique 2: AdaLoRA (Adaptive Low-Rank Adaptation)\n",
        "\n",
        "#### What is AdaLoRA?\n",
        "AdaLoRA dynamically allocates the parameter budget across different weight matrices based on their importance during training.\n",
        "\n",
        "**How it works:**\n",
        "1. Starts with an initial rank (r_init)\n",
        "2. Monitors importance of each adapter during training\n",
        "3. Prunes less important adapters\n",
        "4. Focuses parameters on critical layers\n",
        "\n",
        "#### Key Parameters:\n",
        "- **Initial Rank (init_r)**: 12\n",
        "- **Target Rank (target_r)**: 8\n",
        "- **Beta1/Beta2**: 0.85 (importance smoothing)\n",
        "- **Tinit/Tfinal**: 200/1000 (adaptation schedule)\n",
        "\n",
        "#### Advantages:\n",
        "âœ… Automatic parameter allocation  \n",
        "âœ… More efficient than fixed-rank LoRA  \n",
        "âœ… Adapts to task complexity  \n",
        "âœ… Reduces unnecessary parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f9cbe02-713d-4b1e-87a8-8aeca4ebaa99",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# CELL 6: AdaLoRA Technique Training\n",
        "# ====================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ”¥ TECHNIQUE 2: AdaLoRA (Adaptive LoRA)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Setup W&B for AdaLoRA\n",
        "adalora_config = {\n",
        "    \"technique\": \"AdaLoRA\",\n",
        "    \"model\": \"LLaVA-OneVision\",\n",
        "    \"quantization\": \"4-bit NF4\",\n",
        "    \"init_r\": 12,\n",
        "    \"target_r\": 8,\n",
        "    \"beta1\": 0.85,\n",
        "    \"beta2\": 0.85,\n",
        "    \"tinit\": 200,\n",
        "    \"tfinal\": 1000,\n",
        "    \"samples\": MAX_SAMPLES,\n",
        "    \"epochs\": 4\n",
        "}\n",
        "\n",
        "wandb.init(\n",
        "    project=\"llava-onevision-adalora-weather\",\n",
        "    name=\"weather-satellite-adalora\",\n",
        "    config=adalora_config,\n",
        "    tags=[\"adalora\", \"llava-onevision\", \"weather-satellite\", \"comparison\"]\n",
        ")\n",
        "\n",
        "clear_memory()\n",
        "\n",
        "# Setup quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model for AdaLoRA\n",
        "print(\"ğŸ“¥ Loading LLaVA-OneVision for AdaLoRA...\")\n",
        "model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "\n",
        "# AdaLoRA config\n",
        "try:\n",
        "    peft_config = AdaLoraConfig(\n",
        "        init_r=12,\n",
        "        target_r=8,\n",
        "        beta1=0.85,\n",
        "        beta2=0.85,\n",
        "        tinit=200,\n",
        "        tfinal=1000,\n",
        "        deltaT=10,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "    print(\"âœ… AdaLoRA configuration created\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ AdaLoRA not available, using adaptive LoRA: {e}\")\n",
        "    peft_config = LoraConfig(\n",
        "        lora_alpha=24,\n",
        "        lora_dropout=0.1,\n",
        "        r=12,\n",
        "        bias=\"none\",\n",
        "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "# Apply PEFT\n",
        "peft_model = get_peft_model(model, peft_config)\n",
        "peft_model.print_trainable_parameters()\n",
        "\n",
        "# Training arguments for AdaLoRA\n",
        "adalora_output_dir = \"./llava_adalora_weather_model\"\n",
        "training_args = SFTConfig(\n",
        "    output_dir=adalora_output_dir,\n",
        "    num_train_epochs=4,  # More epochs for adaptation\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    gradient_checkpointing=True,\n",
        "\n",
        "    optim=\"adamw_torch\",\n",
        "    learning_rate=3e-4,  # Higher LR for AdaLoRA\n",
        "    lr_scheduler_type=\"cosine\",  # Cosine schedule for adaptive learning\n",
        "\n",
        "    logging_steps=10,\n",
        "    eval_steps=max(20, len(train_dataset) // 40),\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=max(40, len(train_dataset) // 25),\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    load_best_model_at_end=True,\n",
        "\n",
        "    bf16=True,\n",
        "    tf32=False,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.1,  # More warmup for adaptation\n",
        "\n",
        "    push_to_hub=False,\n",
        "    report_to=\"wandb\",\n",
        "\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    dataset_text_field=\"\",\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
        ")\n",
        "\n",
        "training_args.remove_unused_columns = False\n",
        "\n",
        "# Create trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=create_data_collator_llava(processor),\n",
        "    peft_config=peft_config,\n",
        "    processing_class=processor.tokenizer,\n",
        ")\n",
        "\n",
        "# Train AdaLoRA\n",
        "print(\"ğŸš€ Starting AdaLoRA training...\")\n",
        "adalora_start_time = time.time()\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "adalora_training_time = time.time() - adalora_start_time\n",
        "\n",
        "# Save model locally\n",
        "trainer.save_model(adalora_output_dir)\n",
        "\n",
        "# Final evaluation\n",
        "adalora_eval_result = trainer.evaluate()\n",
        "\n",
        "print(f\"âœ… AdaLoRA training completed in {adalora_training_time/60:.1f} minutes\")\n",
        "print(f\"ğŸ“Š Final AdaLoRA eval loss: {adalora_eval_result['eval_loss']:.4f}\")\n",
        "\n",
        "# Upload to HuggingFace\n",
        "adalora_hf_repo = upload_to_huggingface(adalora_output_dir, \"llava-onevision-weather-adalora\", \"AdaLoRA\")\n",
        "\n",
        "# Log final results\n",
        "wandb.log({\n",
        "    \"final_training_time\": adalora_training_time,\n",
        "    \"final_eval_loss\": adalora_eval_result['eval_loss'],\n",
        "    \"hf_repo\": adalora_hf_repo,\n",
        "    \"training_completed\": True\n",
        "})\n",
        "\n",
        "wandb.finish()\n",
        "\n",
        "print(f\"ğŸ’¾ AdaLoRA model saved locally: {adalora_output_dir}\")\n",
        "if adalora_hf_repo:\n",
        "    print(f\"ğŸŒ AdaLoRA model on HF: https://huggingface.co/{adalora_hf_repo}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0197a9c-e0a7-4b25-bc70-7c99e8b2fd93",
      "metadata": {},
      "source": [
        "### Technique 3: QLoRA (Quantized Low-Rank Adaptation)\n",
        "\n",
        "#### What is QLoRA?\n",
        "QLoRA combines quantization with LoRA to enable fine-tuning of large models on limited hardware.\n",
        "\n",
        "**Key Innovation:**\n",
        "- Base model weights: Quantized to 4-bit (NF4 format)\n",
        "- LoRA adapters: Trained in full precision (16-bit/32-bit)\n",
        "- Result: Massive memory savings with minimal quality loss\n",
        "\n",
        "#### Key Parameters:\n",
        "- **Rank**: 32 (higher rank compensates for quantization)\n",
        "- **Alpha**: 64\n",
        "- **Quantization**: 4-bit NormalFloat (NF4)\n",
        "- **Compute dtype**: bfloat16\n",
        "\n",
        "#### Advantages:\n",
        "âœ… Up to 75% memory reduction  \n",
        "âœ… Enables fine-tuning on consumer GPUs  \n",
        "âœ… Maintains model quality  \n",
        "âœ… Faster training with lower memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e06674f0-e8cc-4272-a907-362f6142511c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# CELL 9: QLoRA Technique Training\n",
        "# ====================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ”¥ TECHNIQUE 3: QLoRA (Quantized LoRA)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Setup W&B for QLoRA\n",
        "qlora_config = {\n",
        "    \"technique\": \"QLoRA\",\n",
        "    \"model\": \"LLaVA-OneVision\",\n",
        "    \"quantization\": \"4-bit NF4\",\n",
        "    \"lora_rank\": 32,  # Higher rank for QLoRA\n",
        "    \"lora_alpha\": 64,\n",
        "    \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],  # More modules\n",
        "    \"samples\": MAX_SAMPLES,\n",
        "    \"epochs\": 3,\n",
        "    \"double_quant\": True,\n",
        "    \"compute_dtype\": \"bfloat16\"\n",
        "}\n",
        "\n",
        "wandb.init(\n",
        "    project=\"llava-onevision-qlora-weather\",\n",
        "    name=\"weather-satellite-qlora\",\n",
        "    config=qlora_config,\n",
        "    tags=[\"qlora\", \"llava-onevision\", \"weather-satellite\", \"comparison\"]\n",
        ")\n",
        "\n",
        "clear_memory()\n",
        "\n",
        "# Setup enhanced quantization for QLoRA\n",
        "qlora_bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,  # Key for QLoRA\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_storage=torch.bfloat16,  # Store quantized weights in bfloat16\n",
        ")\n",
        "\n",
        "# Load model for QLoRA\n",
        "print(\"ğŸ“¥ Loading LLaVA-OneVision for QLoRA...\")\n",
        "model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=qlora_bnb_config,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "\n",
        "# Prepare model for k-bit training\n",
        "from peft import prepare_model_for_kbit_training\n",
        "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
        "\n",
        "# QLoRA config with enhanced settings\n",
        "qlora_peft_config = LoraConfig(\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.1,\n",
        "    r=32,  # Higher rank for better capacity\n",
        "    bias=\"none\",\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",  # Attention\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",     # MLP layers\n",
        "        \"embed_tokens\",                           # Embeddings\n",
        "        \"lm_head\"                                 # Output layer\n",
        "    ],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    modules_to_save=[\"embed_tokens\", \"lm_head\"],  # Save these modules in full precision\n",
        ")\n",
        "\n",
        "# Apply PEFT\n",
        "peft_model = get_peft_model(model, qlora_peft_config)\n",
        "peft_model.print_trainable_parameters()\n",
        "\n",
        "# Training arguments optimized for QLoRA\n",
        "qlora_output_dir = \"./llava_qlora_weather_model\"\n",
        "training_args = SFTConfig(\n",
        "    output_dir=qlora_output_dir,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=16,  # Higher accumulation for QLoRA\n",
        "    gradient_checkpointing=True,\n",
        "\n",
        "    optim=\"paged_adamw_32bit\",  # Paged optimizer for QLoRA\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "\n",
        "    logging_steps=10,\n",
        "    eval_steps=max(30, len(train_dataset) // 25),\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=max(60, len(train_dataset) // 15),\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    load_best_model_at_end=True,\n",
        "\n",
        "    bf16=True,\n",
        "    tf32=False,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.05,\n",
        "    weight_decay=0.001,\n",
        "\n",
        "    push_to_hub=False,\n",
        "    report_to=\"wandb\",\n",
        "\n",
        "    # QLoRA specific settings\n",
        "    ddp_find_unused_parameters=False,\n",
        "    group_by_length=True,\n",
        "\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    dataset_text_field=\"\",\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
        ")\n",
        "\n",
        "training_args.remove_unused_columns = False\n",
        "\n",
        "# Create trainer with QLoRA optimizations\n",
        "trainer = SFTTrainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=create_data_collator_llava(processor),\n",
        "    peft_config=qlora_peft_config,\n",
        "    processing_class=processor.tokenizer,\n",
        ")\n",
        "\n",
        "# Train QLoRA\n",
        "print(\"ğŸš€ Starting QLoRA training...\")\n",
        "qlora_start_time = time.time()\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "qlora_training_time = time.time() - qlora_start_time\n",
        "\n",
        "# Save model locally\n",
        "trainer.save_model(qlora_output_dir)\n",
        "\n",
        "# Final evaluation\n",
        "qlora_eval_result = trainer.evaluate()\n",
        "\n",
        "print(f\"âœ… QLoRA training completed in {qlora_training_time/60:.1f} minutes\")\n",
        "print(f\"ğŸ“Š Final QLoRA eval loss: {qlora_eval_result['eval_loss']:.4f}\")\n",
        "\n",
        "# Upload to HuggingFace\n",
        "qlora_hf_repo = upload_to_huggingface(qlora_output_dir, \"llava-onevision-weather-qlora\", \"QLoRA\")\n",
        "\n",
        "# Log final results\n",
        "wandb.log({\n",
        "    \"final_training_time\": qlora_training_time,\n",
        "    \"final_eval_loss\": qlora_eval_result['eval_loss'],\n",
        "    \"hf_repo\": qlora_hf_repo,\n",
        "    \"training_completed\": True\n",
        "})\n",
        "\n",
        "wandb.finish()\n",
        "\n",
        "print(f\"ğŸ’¾ QLoRA model saved locally: {qlora_output_dir}\")\n",
        "if qlora_hf_repo:\n",
        "    print(f\"ğŸŒ QLoRA model on HF: https://huggingface.co/{qlora_hf_repo}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b2e712d-b117-4f5a-aceb-a2b23bcddd81",
      "metadata": {},
      "source": [
        "## 5. Results & Comparison\n",
        "\n",
        "### Training Comparison Summary\n",
        "Compare the performance, speed, and resource usage of all three techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a040b87-2965-4f52-85a9-0be321aa4d91",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# CELL 7: Comparison Summary\n",
        "# ====================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“Š TRAINING COMPARISON SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nğŸ”µ DoRA Results:\")\n",
        "print(f\"   - Training Time: {dora_training_time/60:.1f} minutes\")\n",
        "print(f\"   - Final Eval Loss: {dora_eval_result['eval_loss']:.4f}\")\n",
        "print(f\"   - Model Path: {dora_output_dir}\")\n",
        "\n",
        "print(f\"\\nğŸŸ¢ AdaLoRA Results:\")\n",
        "print(f\"   - Training Time: {adalora_training_time/60:.1f} minutes\")\n",
        "print(f\"   - Final Eval Loss: {adalora_eval_result['eval_loss']:.4f}\")\n",
        "print(f\"   - Model Path: {adalora_output_dir}\")\n",
        "\n",
        "print(\"\\nâœ… Both models successfully trained and saved!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b38add2-991e-4c5e-9b5d-e6ad1a2b4eaf",
      "metadata": {},
      "source": [
        "## 6. Model Inference & Testing\n",
        "\n",
        "### Test Fine-Tuned Models\n",
        "Run inference with the fine-tuned models to evaluate their performance on sample images.\n",
        "\n",
        "This cell demonstrates:\n",
        "- Loading fine-tuned models\n",
        "- Processing input images\n",
        "- Generating descriptions\n",
        "- Comparing outputs across techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ca041a3-565a-43f0-8f5e-f87b33342537",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# CELL 8: Inference Example (Optional)\n",
        "# ====================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ” INFERENCE EXAMPLE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def run_inference_example(model_path, technique_name):\n",
        "    \"\"\"Run inference with fine-tuned model\"\"\"\n",
        "\n",
        "    print(f\"\\nğŸ¯ Testing {technique_name} model...\")\n",
        "\n",
        "    # Load the fine-tuned model\n",
        "    model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "\n",
        "    # Load adapter\n",
        "    model.load_adapter(model_path)\n",
        "\n",
        "    # Example inference\n",
        "    example_prompt = \"\"\"You are a professional meteorologist analyzing satellite imagery.\n",
        "\n",
        "**SATELLITE ACQUISITION DETAILS:**\n",
        "- Date & Time: January 15, 2025 at 12:00 Noon UTC\n",
        "- Geographic Domain: Morocco, North Africa\n",
        "- Satellite: MODIS Terra\n",
        "\n",
        "Please analyze this satellite image and provide a meteorological analysis.\"\"\"\n",
        "\n",
        "    # Create dummy image for testing (replace with actual image)\n",
        "    test_image = Image.new('RGB', (512, 512), color='blue')\n",
        "\n",
        "    # Format input\n",
        "    conversation = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": example_prompt},\n",
        "                {\"type\": \"image\"}\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Apply chat template\n",
        "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
        "\n",
        "    # Process inputs\n",
        "    inputs = processor(text=prompt, images=[test_image], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9\n",
        "        )\n",
        "\n",
        "    # Decode output\n",
        "    response = processor.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"\\n{technique_name} Response Preview:\")\n",
        "    print(response[:500] + \"...\" if len(response) > 500 else response)\n",
        "\n",
        "    # Clean up\n",
        "    del model\n",
        "    clear_memory()\n",
        "\n",
        "# Test both models\n",
        "if os.path.exists(dora_output_dir):\n",
        "    run_inference_example(dora_output_dir, \"DoRA\")\n",
        "\n",
        "if os.path.exists(adalora_output_dir):\n",
        "    run_inference_example(adalora_output_dir, \"AdaLoRA\")\n",
        "\n",
        "print(\"\\nğŸ‰ All operations completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73614268-7e91-4a66-9de8-c3f55a7c97a4",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary & Conclusions\n",
        "\n",
        "### What We Accomplished\n",
        "âœ… Successfully fine-tuned LLaVA-OneVision for weather satellite analysis  \n",
        "âœ… Implemented and compared three PEFT techniques: DoRA, AdaLoRA, QLoRA  \n",
        "âœ… Evaluated models on weather satellite image descriptions  \n",
        "âœ… Compared performance, memory usage, and training efficiency  \n",
        "\n",
        "\n",
        "**Documentation:**\n",
        "- [LLaVA GitHub](https://github.com/haotian-liu/LLaVA)\n",
        "- [HuggingFace PEFT](https://huggingface.co/docs/peft)\n",
        "- [TRL Documentation](https://huggingface.co/docs/trl)\n",
        "\n",
        "---\n",
        "\n",
        "**Author**: Azeddin sahir \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "LLaVA-finetuning",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 7913507,
          "sourceId": 12535295,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31090,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
